# üöÄ Intel GPU LLM Inference for Linux

> Unlock your Intel Xe GPU for local LLM inference with OpenVINO

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Intel GPU](https://img.shields.io/badge/Intel-Xe%20Graphics-0071C5?logo=intel)](https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html)
[![OpenVINO](https://img.shields.io/badge/OpenVINO-2024-76B900)](https://docs.openvino.ai/)

A complete toolkit for running Large Language Models on **Intel Xe Graphics** (Iris Xe, Arc) using OpenVINO GenAI. Perfect for developers with Intel business laptops who want to run local LLMs efficiently.

## üéØ Why This Project?

- ‚ö° **Performance**: Get 2-3x faster inference vs CPU-only with Intel integrated GPUs
- üíº **Business Laptops**: Works on common Intel Xe laptops (Dell XPS, ThinkPad, HP EliteBook)
- üîí **Privacy**: Run models locally without cloud dependencies
- üìä **Benchmarking**: Built-in tools to measure OpenVINO vs CPU performance
- üéì **Learning**: Complete setup guides and examples for beginners

## üñ•Ô∏è Hardware Requirements

- **GPU**: Intel Xe Graphics (TigerLake, Alderlake, or newer)
  - Examples: Intel Iris Xe, Intel Arc Graphics
- **OS**: Ubuntu 22.04+ (or compatible Debian-based distributions)
- **RAM**: 8GB+ recommended
- **Python**: 3.8+ (for OpenVINO GenAI)

## üìã What's Included

### Setup Scripts

1. **`setup-intel-gpu-llm.sh`** ‚≠ê **Recommended**
   - Sets up OpenVINO GenAI with full Intel GPU support
   - Creates isolated Python virtual environment
   - Installs Intel compute runtime drivers
   - Best for Intel Xe Graphics

2. **`setup-llama-cpp.sh`**
   - Builds llama.cpp for CPU-only inference
   - Used for performance comparison
   - Creates convenience wrapper scripts

3. **`setup-ollama-intel-gpu.sh`**
   - Attempts to configure Ollama for Intel GPU (experimental)
   - Note: Ollama 0.12.8 has limited Intel GPU support
   - Included for reference/testing

4. **`activate-intel-gpu.sh`**
   - Helper script to activate the Python virtual environment
   - Auto-generated by setup script

### Testing & Benchmarking Scripts

1. **`test-inference.py`** 
   - Test individual models with OpenVINO GPU
   - Supports Phi-3, Mistral, Llama 3
   - Flexible CLI with streaming support

2. **`test-models.sh`**
   - Interactive menu for testing models
   - Handles model conversion automatically

3. **`benchmark.py`** ‚≠ê **Performance Comparison**
   - Compare OpenVINO GPU vs llama.cpp CPU
   - Measures tokens/second, latency
   - Generates detailed performance reports

## üöÄ Quick Start

### ‚ö° Fastest Way (Complete Example)

```bash
# Clone repository
git clone <your-repo-url>
cd intel-gpu-llm-inference
git submodule update --init --recursive

# Run setup (one time)
./setup-intel-gpu-llm.sh

# If prompted, log out and back in for group changes to take effect
# Then run the quickstart example
./quickstart-example.sh
```

This downloads TinyLlama (1.1B), converts it to OpenVINO format, and runs a test inference on your Intel GPU. Takes ~10 minutes first time.

### üìñ Step-by-Step Setup

```bash
# 1. Run the setup script
./setup-intel-gpu-llm.sh

# 2. If you were added to the render group, log out and back in

# 3. Activate the environment
source activate-intel-gpu.sh

# 4. Download and convert a model
optimum-cli export openvino --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 tinyllama_ir

# 5. Test inference
python3 << EOF
import openvino_genai as ov_genai
pipe = ov_genai.LLMPipeline("tinyllama_ir", "GPU")
print(pipe.generate("What is artificial intelligence?", max_new_tokens=100))
EOF
```

### Setup CPU Inference (llama.cpp) for Comparison

```bash
# 1. Build llama.cpp
./setup-llama-cpp.sh

# 2. Download a GGUF model (or convert from HuggingFace)
# Download from: https://huggingface.co/models?library=gguf
# Or convert: ./llama-convert microsoft/Phi-3-mini-4k-instruct

# 3. Run inference
./llama-run -m models/phi3-mini.gguf -p "Your prompt here"
```

### Performance Comparison

```bash
# Activate OpenVINO environment
source activate-intel-gpu.sh

# Compare GPU vs CPU performance
./benchmark.py \
  --openvino-model phi3_mini_ir \
  --llama-model models/phi3-mini.gguf \
  --prompt "Explain quantum computing" \
  --max-tokens 100

# Save results
./benchmark.py --compare --output benchmark_results.json
```

## üì¶ What Gets Installed

### System Packages
- Intel OpenCL ICD (GPU compute runtime)
- Intel Level Zero drivers
- Intel oneAPI runtime libraries (if using Ollama script)

### Python Packages (in virtual environment)
- `openvino-genai` - OpenVINO GenAI for LLM inference
- `optimum-intel[openvino]` - Model conversion and optimization tools

## üîß Manual Setup

If you prefer manual installation:

### 1. Install Intel GPU Drivers

```bash
# Add Intel GPU repository
wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | \
    gpg --dearmor | sudo tee /usr/share/keyrings/intel-graphics.gpg > /dev/null

echo "deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy client" | \
    sudo tee /etc/apt/sources.list.d/intel-gpu-jammy.list

sudo apt update
sudo apt install -y intel-opencl-icd intel-level-zero-gpu level-zero
```

### 2. Add User to Render Group

```bash
sudo usermod -aG render $USER
# Log out and back in for changes to take effect
```

### 3. Setup Python Environment

```bash
python3 -m venv openvino_env
source openvino_env/bin/activate
pip install --upgrade pip
pip install openvino-genai optimum-intel[openvino]
```

## üìö Usage Examples

### Testing Models

```bash
# Test Phi-3 Mini on Intel GPU
source activate-intel-gpu.sh
python test-inference.py --model phi3 --prompt "Explain quantum computing"

# Test Mistral 7B with streaming
python test-inference.py --model mistral --prompt "Write a poem" --stream

# Test Llama 3 8B (requires HuggingFace auth)
huggingface-cli login
python test-inference.py --model llama3 --prompt "What is AI?"
```

### Performance Benchmarking

```bash
# Quick comparison
./benchmark.py \
  --openvino-model phi3_mini_ir \
  --llama-model models/phi3-mini-q4.gguf \
  --prompt "Write a story about robots"

# Detailed comparison with multiple runs
./benchmark.py --compare \
  --openvino-model mistral_7b_ir \
  --llama-model models/mistral-7b-q4.gguf \
  --runs 3 \
  --output results.json

# GPU-only benchmark
./benchmark.py --openvino-model phi3_mini_ir --gpu-only
```

### Python Inference Script

```python
import openvino_genai as ov_genai

# Initialize pipeline on GPU
pipe = ov_genai.LLMPipeline("tinyllama_ir", "GPU")

# Generate text
prompt = "Explain quantum computing in simple terms:"
response = pipe.generate(prompt, max_new_tokens=200)
print(response)
```

### Streaming Generation

```python
import openvino_genai as ov_genai

pipe = ov_genai.LLMPipeline("tinyllama_ir", "GPU")

# Streaming callback
def stream_callback(text):
    print(text, end='', flush=True)

config = ov_genai.GenerationConfig()
config.max_new_tokens = 100

pipe.generate("Write a short story:", config, stream_callback)
```

### Download and Convert Models

```bash
# Activate environment
source activate-intel-gpu.sh

# Export from Hugging Face to OpenVINO IR
optimum-cli export openvino --model microsoft/Phi-3-mini-4k-instruct phi3_mini_ir --weight-format int4
optimum-cli export openvino --model mistralai/Mistral-7B-Instruct-v0.2 mistral_7b_ir --weight-format int4

# For llama.cpp (GGUF format)
./llama-convert microsoft/Phi-3-mini-4k-instruct --outfile models/phi3-mini.gguf

# Note: Some models require authentication
huggingface-cli login
```

## üêõ Troubleshooting

### GPU Not Detected

```bash
# Check if GPU is visible
lspci | grep -i vga

# Verify device files
ls -la /dev/dri/

# Check OpenCL detection
clinfo -l

# Verify user permissions
groups | grep render
```

### OpenVINO Import Errors

```bash
# Ensure virtual environment is activated
source openvino_env/bin/activate

# Reinstall if needed
pip install --force-reinstall openvino-genai
```

### Performance Issues

- **Monitor GPU usage**: `intel_gpu_top` (install with `sudo apt install intel-gpu-tools`)
- **Check thermal throttling**: Monitor temperatures with `sensors`
- **Reduce context length**: Use smaller `max_new_tokens` values
- **Try CPU fallback**: Change device from `"GPU"` to `"CPU"` in Python code

## üìÅ Directory Structure

```
.
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ .gitignore                   # Git ignore rules
‚îÇ
‚îú‚îÄ‚îÄ Setup Scripts
‚îú‚îÄ‚îÄ setup-intel-gpu-llm.sh       # OpenVINO GenAI setup (GPU)
‚îú‚îÄ‚îÄ setup-llama-cpp.sh           # llama.cpp build (CPU comparison)
‚îú‚îÄ‚îÄ setup-ollama-intel-gpu.sh    # Ollama setup (experimental)
‚îú‚îÄ‚îÄ activate-intel-gpu.sh        # Environment activation helper
‚îÇ
‚îú‚îÄ‚îÄ Testing & Benchmarking
‚îú‚îÄ‚îÄ test-inference.py            # Test individual models
‚îú‚îÄ‚îÄ test-models.sh               # Interactive model testing
‚îú‚îÄ‚îÄ benchmark.py                 # Performance comparison tool
‚îÇ
‚îú‚îÄ‚îÄ Models & Environments
‚îú‚îÄ‚îÄ openvino_env/                # Python venv (excluded from git)
‚îú‚îÄ‚îÄ *_ir/                        # OpenVINO IR models (excluded from git)
‚îú‚îÄ‚îÄ models/                      # GGUF models for llama.cpp (excluded from git)
‚îî‚îÄ‚îÄ llama.cpp/                   # llama.cpp source & build

```

## ‚ö†Ô∏è Known Limitations

1. **Ollama Intel GPU Support**: As of version 0.12.8, Ollama doesn't include Intel GPU runtime libraries by default
2. **Model Size**: Intel Xe integrated GPUs have limited VRAM (typically shared system RAM)
3. **Performance**: Integrated GPUs are slower than dedicated GPUs but faster than CPU-only inference
4. **Driver Support**: Requires recent Linux kernel (5.15+) for best compatibility

## üîó Resources

- [OpenVINO GenAI Documentation](https://docs.openvino.ai/2025/get-started/install-openvino/install-openvino-genai.html)
- [Intel GPU Drivers for Linux](https://dgpu-docs.intel.com/driver/client/overview.html)
- [Optimum Intel](https://huggingface.co/docs/optimum/intel/index)
- [Hugging Face Models](https://huggingface.co/models)

## ü§ù Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Especially valuable**:
- Hardware test reports (help others know what works)
- Performance benchmarks from different Intel GPU generations
- Bug fixes and documentation improvements
- New model configurations and optimizations

## üìÑ License

MIT License - See [LICENSE](LICENSE) for details.

## üôè Acknowledgments

- Intel for OpenVINO toolkit and GPU drivers
- Hugging Face for model hosting and tools
- OpenVINO community for documentation and support

---

**Note**: This is a community project. For production use, refer to official Intel and OpenVINO documentation.
